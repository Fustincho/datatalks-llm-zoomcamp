{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: open source models with Ollama\n",
    "\n",
    "In this notebook, we will utilize open-source LLMs via Ollama, a robust and user-friendly platform designed to run LLMs locally. Ollama simplifies downloading, installing, and interacting with various LLMs, enabling offline access and reliable performance in areas with limited internet connectivity. Additionally, it offers a local API for seamless integration into applications and workflows, making it an excellent tool for experimentation, learning, and practical AI deployment.\n",
    "\n",
    "To install Ollama locally, we will use Docker to pull the official container image from Docker Hub. Execute the following command:\n",
    "\n",
    "```bash\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "```\n",
    "\n",
    "The `/root/.ollama` folder contains information about the models stored in Ollama after downloading them. We are using a named volume for this directory because it allows us to attach the same volume to another container running Ollama, ensuring consistent model storage and access across different containers.\n",
    "\n",
    "After executing the former command, we will check if the container is active by checking the Ollama version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.1.48\n"
     ]
    }
   ],
   "source": [
    "!docker exec ollama ollama -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download a smaller LLM called gemma:2b. To do this, we enter the Ollama container and pull the model with the following command:\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "In Docker, the results are saved into `/root/.ollama`. For the sake of this homework, we are particularly interested in the metadata about this model, which can be found in `models/manifests/registry.ollama.ai/library`.\n",
    "\n",
    "For a list of available models, visit [Ollama's model library](https://ollama.com/library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted JSON Output:\n",
      "{\n",
      "    \"schemaVersion\": 2,\n",
      "    \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n",
      "    \"config\": {\n",
      "        \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n",
      "        \"digest\": \"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\n",
      "        \"size\": 483\n",
      "    },\n",
      "    \"layers\": [\n",
      "        {\n",
      "            \"mediaType\": \"application/vnd.ollama.image.model\",\n",
      "            \"digest\": \"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\n",
      "            \"size\": 1678447520\n",
      "        },\n",
      "        {\n",
      "            \"mediaType\": \"application/vnd.ollama.image.license\",\n",
      "            \"digest\": \"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\n",
      "            \"size\": 8433\n",
      "        },\n",
      "        {\n",
      "            \"mediaType\": \"application/vnd.ollama.image.template\",\n",
      "            \"digest\": \"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\n",
      "            \"size\": 136\n",
      "        },\n",
      "        {\n",
      "            \"mediaType\": \"application/vnd.ollama.image.params\",\n",
      "            \"digest\": \"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\n",
      "            \"size\": 84\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "_ = subprocess.run(\"docker exec ollama ollama pull gemma:2b\".split(),\n",
    "                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "result = subprocess.run(\n",
    "    \"docker exec ollama cat /root/.ollama/models/manifests/registry.ollama.ai/library/gemma/2b\".split(), \n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Get the standard output\n",
    "gemma_metadata = result.stdout\n",
    "# Parse the JSON string\n",
    "gemma_metadata = json.loads(gemma_metadata)\n",
    "\n",
    "# Print the formatted JSON\n",
    "print(\"Formatted JSON Output:\")\n",
    "print(json.dumps(gemma_metadata, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test this LLM by executing `ollama run gemma:2b`. This allows us to start communicating with the model and ask questions to it. Here's an example where we ask the model what's ten times ten:\n",
    "\n",
    "![](./img/question_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to create the container, instead of using a named volume, is through bind mounts. This approach provides more control over where the files are stored locally, rather than using the default location of a named volume.\n",
    "\n",
    "A practical use case for this method is to have a specific folder where the model data is stored after executing ollama pull. This is particularly useful when creating custom containers for Ollama. Instead of downloading the model each time and relying on the Ollama library, you can copy the files you initially downloaded, which were saved thanks to the bind mount.\n",
    "\n",
    "Let's run a container with this new configuration:\n",
    "\n",
    "First, create a directory to store the files:\n",
    "\n",
    "```bash\n",
    "mkdir ollama_files\n",
    "```\n",
    "Then, run the Docker container with the bind mount:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "\n",
    "docker exec -it ollama ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "We will now use the du (disk usage) command to check the size of the model we just downloaded. We will use the --si option to display sizes in powers of 1000 (rather than 1024), and the -s option to summarize the size of the folder without listing individual file sizes.\n",
    "\n",
    "```bash\n",
    "du --si -s ollama_files/models\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7G\tollama_files/models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = subprocess.run(\"du --si -s ollama_files/models\".split(), capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have downloaded the files for this model, we can create our own Ollama-based container and copy the model directly into it. To do this, we will create a Dockerfile that copies these files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM ollama/ollama\n",
    "\n",
    "COPY ./ollama_files /root/.ollama\n",
    "\n",
    "EXPOSE 11434\n",
    "\n",
    "CMD [\"serve\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the `serve` command here for readability, as it is the default command used by Ollama. This can be verified in the original image by executing `docker inspect`. Let's now build the custom image:\n",
    "\n",
    "```bash\n",
    "docker build -t ollama-gemma2b .\n",
    "```\n",
    "\n",
    "And run it:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 11434:11434 ollama-gemma2b\n",
    "```\n",
    "\n",
    "We can interact with the model using the OpenAI library to connect to the Ollama API. Here is an example of how to set up the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What's the formula for energy?\"\n",
    "response = client.chat.completions.create(\n",
    "        model='gemma:2b',\n",
    "        temperature=0,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the formula for energy:\n",
      "\n",
      "**E = K + U**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **E** is the energy in joules (J)\n",
      "* **K** is the kinetic energy in joules (J)\n",
      "* **U** is the potential energy in joules (J)\n",
      "\n",
      "**Kinetic energy (K)** is the energy an object possesses when it moves or is in motion. It is calculated as half the product of an object's mass (m) and its velocity (v) squared:\n",
      "\n",
      "**K = 1/2mv^2**\n",
      "\n",
      "**Potential energy (U)** is the energy an object possesses due to its position or configuration. It is calculated as the product of an object's mass, gravitational constant (g), and height or position above a reference point.\n",
      "\n",
      "**U = mgh**\n",
      "\n",
      "**Where:**\n",
      "\n",
      "* **m** is the mass in kilograms (kg)\n",
      "* **g** is the acceleration due to gravity in meters per second squared (m/s²)\n",
      "* **h** is the height or position in meters (m)\n",
      "\n",
      "The formula shows that energy can be expressed as the sum of kinetic and potential energy. The kinetic energy is a measure of the object's ability to do work, while the potential energy is a measure of the object's ability to do work against gravity.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=283, prompt_tokens=34, total_tokens=317)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
