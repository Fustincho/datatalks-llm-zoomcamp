{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: vector search\n",
    "\n",
    "In this notebook, we will explore vector search using embeddings with and without Elasticsearch. The process involves obtaining embeddings for a given query and a set of documents, performing similarity searches, and evaluating the effectiveness of the search engine using metrics such as hit-rate. We will also compare the performance of our custom vector search engine against Elasticsearch. This exercise aims to provide practical insights into implementing and evaluating vector search techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers\n",
    "\n",
    "In this step, we import the `SentenceTransformer` class from the `sentence_transformers` library. We then load a pre-trained model named `multi-qa-distilbert-cos-v1` which is specifically designed for question-answering tasks. This model will be used to generate embeddings for our queries and documents, allowing us to perform vector search based on the similarity of these embeddings.\n",
    "\n",
    "Check the [Senterce Transformers' model library](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html) if you're interested in checking out other models that are available. \n",
    "\n",
    "This is the information of `multi-qa-distilbert-cos-v1`:\n",
    "\n",
    "| **Attribute**             | **Details**                                                                                                                                                                    |\n",
    "|---------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Description**           | This model was tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs.        |\n",
    "| **Base Model**            | distilbert-base                                                                                                                                                                |\n",
    "| **Max Sequence Length**   | 512                                                                                                                                                                            |\n",
    "| **Dimensions**            | 768                                                                                                                                                                            |\n",
    "| **Normalized Embeddings** | true                                                                                                                                                                           |\n",
    "| **Suitable Score Functions** | dot-product (util.dot_score), cosine-similarity (util.cos_sim), euclidean distance                                                                                          |\n",
    "| **Size**                  | 250 MB                                                                                                                                                                         |\n",
    "| **Pooling**               | Mean Pooling                                                                                                                                                                   |\n",
    "| **Training Data**         | 215M (question, answer) pairs from diverse sources.                                                                                                                            |\n",
    "| **Model Card**            | [https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1)                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"multi-qa-distilbert-cos-v1\"\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This embedding has a dimensionality of 768. The first values of this vector are [ 0.07822261 -0.04013115  0.03861361 -0.00017896  0.08923467]\n"
     ]
    }
   ],
   "source": [
    "query = \"I just discovered the course. Can I still join it?\"\n",
    "\n",
    "query_embedding = embedding_model.encode(query)\n",
    "print(f\"This embedding has a dimensionality of {len(query_embedding)}. The first values of this vector are {query_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading \n",
    "\n",
    "Now we will download the documents we will use for this homework. We also save a copy of these documents for reproductibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'id': 'c02e79ef'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/documents-with-ids.json'\n",
    "docs_url = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "docs_response = requests.get(docs_url)\n",
    "documents = docs_response.json()\n",
    "\n",
    "# Save the fetched documents locally as documents.json\n",
    "with open('documents-with-ids.json', 'w') as f:\n",
    "    json.dump(documents, f)\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use only a subset of the questions: the questions belonging to \"machine-learning-zoomcamp\".\n",
    "ml_documents = [document for document in documents if document['course'] == \"machine-learning-zoomcamp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the embeddings\n",
    "\n",
    "In this step, we generate embeddings for each document using the pre-trained model. These vectors are stored in a list, which is then converted to a numpy array for efficient numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:32<00:00, 11.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(375, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "embeddings = []\n",
    "for document in tqdm(ml_documents):\n",
    "    qa_text = f\"{document['question']} {document['text']}\"\n",
    "    embeddings.append(embedding_model.encode(qa_text))\n",
    "X = np.array(embeddings)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we compute the dot product of the first embedding vector with itself to show that the vectors returned from the embedding model are already normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product = X[0].dot(X[0])\n",
    "dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the similarity between the query embedding and all document embeddings using the dot product. We then identify the document with the highest similarity score by iterating through the computed similarities and finding the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #15 has the highest similarity with a dot product of 0.6506574153900146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'The course has already started. Can I still join it?',\n",
       " 'course': 'machine-learning-zoomcamp',\n",
       " 'id': 'ee58a693'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = X.dot(query_embedding)\n",
    "highest_similarity = max(enumerate(similarities), key=lambda x: x[1])\n",
    "print(f\"Document #{highest_similarity[0] + 1} has the highest similarity with a dot product of {highest_similarity[1]}\")\n",
    "ml_documents[highest_similarity[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Search Engine\n",
    "\n",
    "Now we define a `VectorSearchEngine` class to facilitate efficient document retrieval based on vector similarity. The class takes a list of documents and their corresponding embeddings as inputs during initialization. The `searc`h` method computes similarity scores between a query embedding and all document embeddings using the dot product.\n",
    "\n",
    "To efficiently find the top `num_results` scores, we use `np.argpartition` which is more efficient than `np.argsort` when we only need the top results sorted. This improves the search performance by avoiding a full sort of all scores. In the appendix, we discuss the differences and efficiencies of `np.argpartition` and `np.argsort` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'The course has already started. Can I still join it?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ee58a693'},\n",
       " {'text': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'I just joined. What should I do next? How can I access course materials?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0a278fb2'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorSearchEngine():\n",
    "    def __init__(self, documents, embeddings):\n",
    "        self.documents = documents\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, v_query, num_results=10):\n",
    "        scores = self.embeddings.dot(v_query)\n",
    "        partition_idx = np.argpartition(scores, -num_results)[-num_results:]\n",
    "        idx = partition_idx[np.argsort(-scores[partition_idx])]\n",
    "        return [self.documents[i] for i in idx]\n",
    "\n",
    "search_engine = VectorSearchEngine(documents=ml_documents, embeddings=X)\n",
    "search_engine.search(query_embedding, num_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth data\n",
    "\n",
    "In this section, we load the ground truth data for the evaluation of our search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/ground-truth-data.csv'\n",
    "ground_truth_url = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "\n",
    "df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "df_ground_truth.to_json('ground_truth.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains relevant questions and the corresponding document IDs that answer these questions. Our strategy is to generate embeddings for all questions and evaluate the search engine's performance by checking if the top 5 retrieved results contain the ground truth document ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Where can I sign up for the course?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'document': '0227b872'},\n",
       " {'question': 'Can you provide a link to sign up?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'document': '0227b872'},\n",
       " {'question': 'Is there an FAQ for this Machine Learning course?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'document': '0227b872'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "The ground truth data will be used to calculate the evaluation metrics. Here we define the evaluation functions and describe their purpose:\n",
    "\n",
    "**Hit Rate (HR) or Recall at k:**\n",
    "\n",
    "Measures the proportion of queries for which at least one relevant document is retrieved in the top k results.\n",
    "\n",
    "$$\n",
    "HR@k = \\frac{\\text{Number of queries with at least one relevant document in top k}}{|Q|}\n",
    "$$\n",
    "\n",
    "**Mean Reciprocal Rank (MRR):**\n",
    "\n",
    "Evaluates the rank position of the first relevant document.\n",
    "\n",
    "$$\n",
    "MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\left( \\frac{1}{\\text{rank}_i} \\right)\n",
    "$$\n",
    "\n",
    "where $ |Q| $ is the total number of queries and $ \\text{rank}_i $ is the rank position of the first relevant document for the i-th query.\n",
    "\n",
    "`evaluate` performs the embedding of the query question, uses the search function to retrieve the top results, and checks if the ground truth document is among the retrieved results. It creates a list of booleans indicating matches for each document and calculates the HR and MRR metrics based on these matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(matches_list):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in matches_list:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(matches_list)\n",
    "\n",
    "def mrr(matches_list):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in matches_list:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(matches_list)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    matches_list = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        matches = [d['id'] == doc_id for d in results]\n",
    "        matches_list.append(matches)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(matches_list),\n",
    "        'mrr': mrr(matches_list),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `search` function takes an entry from the ground truth as input, encodes the question using the embedding model, and then uses the search engine to retrieve the top 5 results based on the vector similarity. We then use the evaluate function to assess the performance of our search engine using the ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [00:41<00:00, 43.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.9398907103825137, 'mrr': 0.8516484517304189}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(q):\n",
    "    question = q['question']\n",
    "    v_q = embedding_model.encode(f\"{question}\")\n",
    "\n",
    "    return search_engine.search(v_q, num_results=5)\n",
    "\n",
    "evaluate(ground_truth, search_function=search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching using Elasticsearch\n",
    "\n",
    "Now we will perform the search with Elasticsearch. Make sure you have a container running Elasticsearch before proceeding. For a brief intro to Elasticsearch and an example of running such a container is found [here](https://github.com/Fustincho/datatalks-llm-zoomcamp/blob/main/01-intro/elasticsearch.ipynb).\n",
    "\n",
    "We will index the `ml_documents`. For this we must define the mappings for every key found in our documents. We will also add a new key, `question_text_vector` in which we will add the embedding vector before indexing it into Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'ml_questions'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"question_text_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 768,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = 'ml_questions'\n",
    "\n",
    "# Delete the index if it already exists\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "es.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:01<00:00, 286.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ml_documents)):\n",
    "    ml_documents[i][\"question_text_vector\"] = X[i]\n",
    "    \n",
    "for doc in tqdm(ml_documents):\n",
    "    es.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we define a function `elastic_search_knn` to perform k-Nearest Neighbors (k-NN) search using Elasticsearch. The function takes a field name and a query vector as inputs. It constructs a search query with the specified field and query vector, specifying the number of nearest neighbors (k) to return and the number of candidates to consider (`num_candidates`). The search query is executed against the Elasticsearch index, and the resulting documents are extracted and returned as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_knn(field, query_vector):\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": {\n",
    "            \"field\": field,\n",
    "            \"query_vector\": query_vector,\n",
    "            \"k\": 5,\n",
    "            \"num_candidates\": 10000,\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"text\",\n",
    "            \"section\",\n",
    "            \"question\",\n",
    "            \"course\",\n",
    "            \"id\",\n",
    "        ],  # Specifies what elements from the source document should be returned\n",
    "    }\n",
    "\n",
    "    es_results = es.search(index=index_name, body=search_query)\n",
    "\n",
    "    result_docs = []\n",
    "\n",
    "    for hit in es_results[\"hits\"][\"hits\"]:\n",
    "        result_docs.append(hit[\"_source\"])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'The course has already started. Can I still join it?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'id': 'ee58a693'},\n",
       " {'question': 'I just joined. What should I do next? How can I access course materials?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'text': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'id': '0a278fb2'},\n",
       " {'question': \"I filled the form, but haven't received a confirmation email. Is it normal?\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'text': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'id': '6ba259b1'},\n",
       " {'question': 'Can I do the course in other languages, like R or Scala?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'section': 'Miscellaneous',\n",
       "  'text': 'Technically, yes. Advisable? Not really. Reasons:\\nSome homework(s) asks for specific python library versions.\\nAnswers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\\nAnd as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\\nYou can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.\\ntx[source]',\n",
       "  'id': '9f261648'},\n",
       " {'question': 'The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'text': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'id': 'e7ba6b8a'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_search_knn(\"question_text_vector\", query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the search function `search_es` uses Elasticsearch for retrieving the top results. The function takes a query as input, encodes the question using the embedding model, and performs a k-NN search using the `elastic_search_knn` function.\n",
    "\n",
    "We then evaluate the search engine's performance using the ground truth data and the evaluate function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [00:29<00:00, 61.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.9398907103825137, 'mrr': 0.8516484517304189}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_es(q):\n",
    "    question = q['question']\n",
    "    v_q = embedding_model.encode(f\"{question}\")\n",
    "\n",
    "    return elastic_search_knn(\"question_text_vector\", v_q)\n",
    "\n",
    "evaluate(ground_truth=ground_truth, search_function=search_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1: Finding Indices of Highest Values for Vector Similarities in Search Engine\n",
    "\n",
    "When implementing a search engine that ranks results based on vector similarities, it is often necessary to find the indices of the highest similarity values. Depending on the number of top results you need, different functions in `numpy` can be used efficiently.\n",
    "\n",
    "## 1. Using `numpy.argmax`\n",
    "- **Purpose**: Find the index of the single highest similarity value in an array.\n",
    "- **Time Complexity**: `O(n)`\n",
    "- **Best Use Case**: When you need to find the index of the single highest similarity value.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  import numpy as np\n",
    "\n",
    "  similarities = np.array([0.3, 0.1, 0.2, 0.5, 0.4])\n",
    "  max_index = np.argmax(similarities)\n",
    "  ```\n",
    "\n",
    "## 2. Using `numpy.argsort`\n",
    "- **Purpose**: Find the indices of the sorted array of similarity values.\n",
    "- **Time Complexity**: `O(n log n)`\n",
    "- **Best Use Case**: When you need a fully sorted array or a sorted order of similarity values, including their indices.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  import numpy as np\n",
    "\n",
    "  similarities = np.array([0.3, 0.1, 0.2, 0.5, 0.4])\n",
    "  sorted_indices = np.argsort(similarities)\n",
    "  top_k_indices = sorted_indices[-5:]  # For top 5 similarity values\n",
    "  ```\n",
    "\n",
    "## 3. Using `numpy.argpartition`\n",
    "- **Purpose**: Find the indices of the top k highest similarity values.\n",
    "- **Time Complexity**: `O(n)` for partitioning + `O(k log k)` for sorting the top k elements.\n",
    "- **Best Use Case**: When you need the top k highest similarity values efficiently, especially for large arrays and small k.\n",
    "- **Example**:\n",
    "  ```python\n",
    "  import numpy as np\n",
    "\n",
    "  similarities = np.array([0.3, 0.1, 0.2, 0.5, 0.4, 0.7, 0.6, 0.8, 0.9, 0.0])\n",
    "  k = 5\n",
    "  partitioned_indices = np.argpartition(similarities, -k)[-k:]\n",
    "  top_k_indices = partitioned_indices[np.argsort(-similarities[partitioned_indices])]\n",
    "  ```\n",
    "np.argpartition rearranges the indices of the array such that all values before the value at the k-th position are less than or equal to it, and all values after are greater than or equal to it. This operation ensures that the k-th largest element is in its final sorted position, but the order of the elements in the two partitions (before and after the k-th element) is not guaranteed to be sorted.\n",
    "\n",
    "In the example, `np.argpartition(similarities, -k)` partitions the array similarities such that the indices of the top k highest values are placed in the last k positions of the array.\n",
    "`partitioned_indices = np.argpartition(similarities, -k)[-k:]` retrieves these indices.\n",
    "\n",
    "`np.argsort(-similarities[partitioned_indices])` sorts these top k indices based on the actual similarity values in descending order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
